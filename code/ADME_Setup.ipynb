{"cells":[{"cell_type":"markdown","id":"88b079b6-2096-472b-9542-747a096d76af","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Setup code for main data table, logging table and run info table\n","\n","\n","Below is code to create the tables needed to export data from ADME/OSDU to Fabric. Use the variables \"table_name\", \"logging_table\" and \"run_info_table\" to give names to the tables. Run the setup code below first, the cell with the schemas, and then the cell that creates the table you want. Notice the setting \"delete_existing_table\". It is default set to \"no\" which means the code will do nothing if the table exists. If this is changed to yes existing table with the same name will be deleted and emptied.\n","\n","## Setup variables for ADME\n","server - They URL to the ADME server. You can find this in the portal on the overview page of the ADME instance\n","\n","Api URLs - These you do not have to change and can use the values already there.\n","\n","data_partition - The name of the data partition you want to export from. Data partitions can be found in the Portal on the ADME instance. Find data partitions on the left side when you have opened the ADME instance.\n","\n","legal_tag - This is the default value that will be put on the exported document if the original does not have a legal_tag\n","\n","acl_viewer - This is the default value that will be put on the exported document if the original does not have a value\n","\n","acl_owner - This is the default value that will be put on the exported document if the original does not have a value\n","\n","authentication_mode - In this example we use \"msal_interactive\" See (insert link here) for more information\n","\n","authority - \"https://login.microsoftonline.com/xxxxx\" where xxxx is your tenantid\n","\n","scopes - [\"xxxx/.default\"], where xxxx is your client_id. NOTE this variable is a list and therefore it needs the square brackets even if it is only one value\n","\n","client_id - this is the app id that was used to create the ADME instance\n","\n","tenant_id - the tenant id. Search Tenant properties in Portal to find this value. This is the tenant where ADME resides.\n","\n","redirect_uri - this value is set on the app used when creating the ADME instance. \n","\n","access_token_type - \"keyvault\", it is strongly recommended that you use a keyvault for the key to access AMDE\n","\n","key_vault_name - the name of the key vault\n","\n","secret_name - the name of the secret in the key vault\n","\n","table_name - the name of the table you store data to in Fabric - put code at bottom of notebook\n","\n","logging_table - the name of the table where logs will be stored\n","\n","run_info_table - the name of the table where last run info is stored. This is used for delta loads since last run time\n","\n","lakehouse_name - the name of the lakehouse where the delta tables will reside\n","\n","## How to use the variables\n","\n","**The variables are called with config[\"variable\"], for example: config[\"server\"]**\n"]},{"cell_type":"code","execution_count":null,"id":"a433877a-c1cf-44c9-a387-acd66d9aff91","metadata":{"jupyter":{"outputs_hidden":true,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","# Correct the JSON string format and load it\n","config_json = '''\n","{\n","    \"server\": \"\",\n","    \"crs_catalog_url\": \"/api/crs/catalog/v2/\",\n","    \"crs_converter_url\": \"/api/crs/converter/v2/\",\n","    \"entitlements_url\": \"/api/entitlements/v2/\",\n","    \"file_url\": \"/api/file/v2/\",\n","    \"legal_url\": \"/api/legal/v1/\",\n","    \"schema_url\": \"/api/schema-service/v1/\",\n","    \"search_url\": \"/api/search/v2/\",\n","    \"storage_url\": \"/api/storage/v2/\",\n","    \"unit_url\": \"/api/unit/v3/\",\n","    \"workflow_url\": \"/api/workflow/v1/\",\n","    \"data_partition_id\": \"\",\n","    \"legal_tag\": \"legal_tag\",\n","    \"acl_viewer\": \"acl_viewer\",\n","    \"acl_owner\": \"acl_owner\",\n","    \"authentication_mode\": \"msal_interactive\",\n","    \"authority\": \"\",\n","    \"scopes\": [\"\"],\n","    \"client_id\": \"\",\n","    \"tenant_id\": \"\",\n","    \"redirect_uri\": \"\",\n","    \"access_token_type\" : \"keyvault\",\n","    \"key_vault_name\" : \"\",\n","    \"secret_name\" : \"\",\n","    \"table_name\" : \"main\",\n","    \"logging_table\" : \"logging_info\",\n","    \"run_info_table\" : \"run_info\",\n","    \"delete_existing_table\" : \"no\"\n","}\n","'''\n","\n","# Load the JSON string into a Python dictionary\n","config_dict = json.loads(config_json)\n","\n","# Create a Series from the dictionary\n","config = pd.Series(config_dict)\n","\n","#The number of documents in each batch. If you increase this you could see error messages about the load being too big\n","batch_size = 750\n","\n","display(config)"]},{"cell_type":"code","execution_count":null,"id":"9be7f400-a193-455f-8371-aa070def16fb","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, MapType\n","\n","# Define the log schema\n","log_schema = StructType([\n","    StructField(\"log_id\", StringType(), False),\n","    StructField(\"log_timestamp\", TimestampType(), False),\n","    StructField(\"log_level\", StringType(), False),\n","    StructField(\"file_name\", StringType(), False),\n","    StructField(\"line_number\", StringType(), False),\n","    StructField(\"message\", StringType(), False)\n","])\n","\n","# Define the data schema\n","schema = StructType([\n","    StructField(\"createTime\", StringType(), True),\n","    StructField(\"kind\", StringType(), True),\n","    StructField(\"authority\", StringType(), True),\n","    StructField(\"namespace\", StringType(), True),\n","    StructField(\"legal\", MapType(StringType(), StringType()), True),\n","    StructField(\"createUser\", StringType(), True),\n","    StructField(\"source\", StringType(), True),\n","    StructField(\"acl\", MapType(StringType(), StringType()), True),\n","    StructField(\"id\", StringType(), True),\n","    StructField(\"type\", StringType(), True),\n","    StructField(\"version\", StringType(), True),\n","    StructField(\"tags\", MapType(StringType(), StringType()), True),\n","    StructField(\"data\", MapType(StringType(), StringType()), True),\n","    StructField(\"modifyUser\", StringType(), True),\n","    StructField(\"modifyTime\", StringType(), True),\n","    StructField(\"ancestry\", MapType(StringType(), StringType()), True),\n","    StructField(\"ingestTime\", StringType(), True)\n","])\n","\n","run_info_schema = StructType([\n","    StructField(\"run_id\", StringType(), False),\n","    StructField(\"run_timestamp\", LongType(), False)\n","])"]},{"cell_type":"code","execution_count":null,"id":"3552e107-e18d-4371-89ca-2a2b7c2de326","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Run this to create logging_info table\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","from delta.tables import DeltaTable\n","from pyspark.sql.functions import current_timestamp, lit\n","from datetime import datetime\n","import uuid\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"LoggingTableCreation\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Define the target table name and path\n","table_name = config[\"logging_table\"]\n","table_path = f\"Tables/{table_name}\"\n","\n","# Check the config for whether to delete the existing table\n","delete_existing_table = config.get(\"delete_existing_table\", \"no\").lower() == \"yes\"\n","\n","# Check if the table exists\n","table_exists = DeltaTable.isDeltaTable(spark, table_path)\n","\n","if table_exists and delete_existing_table:\n","    # If table exists and we need to delete/overwrite it\n","    deltaTable = DeltaTable.forPath(spark, table_path)\n","    deltaTable.delete()  # This deletes all records in the table, not the table itself\n","    \n","    # Create an empty DataFrame with the schema and overwrite the existing table\n","    empty_df = spark.createDataFrame([], log_schema)\n","    empty_df.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n","    print(f\"Existing table {table_name} at {table_path} was deleted and recreated with the schema.\")\n","\n","elif not table_exists:\n","    # If the table does not exist, create it\n","    empty_df = spark.createDataFrame([], log_schema)\n","    empty_df.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n","    print(f\"Table {table_name} created at {table_path} with the schema.\")\n","else:\n","    # If the table exists and we should not delete it\n","    print(f\"Table {table_name} already exists at {table_path}. No changes made.\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"cdd7941b-e3f1-400b-beb6-ce412b36168d","metadata":{"jupyter":{"outputs_hidden":true,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Run this code to create the run_info table before batch export\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","from delta.tables import DeltaTable\n","from pyspark.sql.functions import current_timestamp, lit\n","from datetime import datetime\n","import uuid\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"RunInfoTableCreation\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Define the target table name and path\n","table_name = config[\"run_info_table\"]\n","table_path = f\"Tables/{table_name}\"\n","print(table_path)\n","\n","# Check the config for whether to delete the existing table\n","delete_existing_table = config.get(\"delete_existing_table\", \"no\").lower() == \"yes\"\n","\n","# Check if the table exists\n","table_exists = DeltaTable.isDeltaTable(spark, table_path)\n","\n","if table_exists and delete_existing_table:\n","    # If table exists and we need to delete/overwrite it\n","    deltaTable = DeltaTable.forPath(spark, table_path)\n","    deltaTable.delete()  # This deletes all records in the table, not the table itself\n","    \n","    # Create an empty DataFrame with the schema and overwrite the existing table\n","    empty_df = spark.createDataFrame([], run_info_schema)\n","    empty_df.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n","    print(f\"Existing table {table_name} at {table_path} was deleted and recreated with the schema.\")\n","\n","elif not table_exists:\n","    # If the table does not exist, create it\n","    empty_df = spark.createDataFrame([], run_info_schema)\n","    empty_df.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n","    print(f\"Table {table_name} created at {table_path} with the schema.\")\n","else:\n","    # If the table exists and we should not delete it\n","    print(f\"Table {table_name} already exists at {table_path}. No changes made.\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"dc731b96-296e-410e-8ee6-aaf18987c3ec","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Run this code to create the main table before batch export\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","from delta.tables import DeltaTable\n","from pyspark.sql.functions import current_timestamp, lit\n","from datetime import datetime\n","import uuid\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"LoggingTableCreation\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Define the target table name and path\n","table_name = config[\"table_name\"]\n","table_path = f\"Tables/{table_name}\"\n","\n","# Check the config for whether to delete the existing table\n","delete_existing_table = config.get(\"delete_existing_table\", \"no\").lower() == \"yes\"\n","\n","# Check if the table exists\n","table_exists = DeltaTable.isDeltaTable(spark, table_path)\n","\n","if table_exists and delete_existing_table:\n","    # If table exists and we need to delete/overwrite it\n","    deltaTable = DeltaTable.forPath(spark, table_path)\n","    deltaTable.delete()  # This deletes all records in the table, not the table itself\n","    \n","    # Create an empty DataFrame with the schema and overwrite the existing table\n","    empty_df = spark.createDataFrame([], schema)\n","    empty_df.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n","    print(f\"Existing table {table_name} at {table_path} was deleted and recreated with the schema.\")\n","\n","elif not table_exists:\n","    # If the table does not exist, create it\n","    empty_df = spark.createDataFrame([], schema)\n","    empty_df.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n","    print(f\"Table {table_name} created at {table_path} with the schema.\")\n","else:\n","    # If the table exists and we should not delete it\n","    print(f\"Table {table_name} already exists at {table_path}. No changes made.\")\n","\n"]},{"cell_type":"markdown","id":"eb31bce8-db53-49f5-b225-446adda9d529","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Helper code\n","Below are some cells with code to help developers check imported data and similar QA tasks"]},{"cell_type":"code","execution_count":null,"id":"3ef54abc-dfb3-407d-8011-68238b1341c9","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["#Code to see imported data - copy this into a cell at the bottom of the batch export to check the number of documents in the lakehouse\n","# Insert the current timestamp\n","run_id = str(uuid.uuid4())\n","run_info_df = spark.createDataFrame([(run_id,)], [\"run_id\"])\n","run_info_df = run_info_df.withColumn(\"run_timestamp\", current_timestamp())\n","run_info_df.write.insertInto(\"admelakehouse.run_info\", overwrite=False)\n","\n","# Display the 10 documents with the newest createTime\n","df_newest = spark.sql(f\"SELECT * FROM admelakehouse.{table_name} ORDER BY createTime DESC LIMIT 10\")\n","display(df_newest)\n","\n","# Query to select all rows from the table\n","df_all = spark.sql(f\"SELECT * FROM admelakehouse.{table_name}\")\n","\n","# Count the number of rows\n","num_documents = df_all.count()\n","\n","# Print the number of documents\n","print(f\"Number of documents in admelakehouse.{table_name}: {num_documents}\")"]},{"cell_type":"code","execution_count":null,"id":"8e31aabd-a420-43e9-a58a-e8f23f3ebc10","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Testcode to see how many documents has been imported and checking for duplicates\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, count\n","from delta.tables import DeltaTable\n","\n","spark = SparkSession.builder \\\n","    .appName(\"Table\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .getOrCreate()\n","\n","# Define the target table name and path\n","table_name = config[\"table_name\"]\n","target_table_path = f\"Tables/{table_name}\"\n","\n","# Load the Delta table\n","bronze_table = DeltaTable.forPath(spark, target_table_path)\n","\n","# Read the data into a DataFrame\n","df = bronze_table.toDF()\n","\n","# Count the occurrences of each id\n","id_counts = df.groupBy(\"id\").agg(count(\"id\").alias(\"count\"))\n","\n","# Filter for duplicate ids (count > 1)\n","duplicate_ids = id_counts.filter(col(\"count\") > 1)\n","\n","# Show duplicate ids if any\n","if duplicate_ids.count() > 0:\n","    print(\"Duplicate IDs found:\")\n","    duplicate_ids.show(truncate=False)\n","else:\n","    print(\"All IDs are unique.\")\n","\n","# Query to select all rows from the table\n","df_all = spark.sql(\"SELECT * FROM admelakehouse.main\")\n","\n","# Count the number of rows\n","num_documents = df_all.count()\n","\n","# Print the number of documents\n","print(f\"Number of documents in admelakehouse.main: {num_documents}\")\n","\n","# Query to select all rows from the table\n","df_all_run = spark.sql(\"SELECT * FROM admelakehouse.run_info\")\n","\n","# Count the number of rows\n","num_documents_run = df_all_run.count()\n","\n","# Print the number of documents\n","print(f\"Number of documents in admelakehouse.run_info: {num_documents_run}\")\n"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"1754e36b-3b11-46b5-9e3e-6d0ff96d2511","default_lakehouse_name":"admelakehouse","default_lakehouse_workspace_id":"e7835792-0470-4896-8a4c-0935e39ffc5c"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"synapse_pyspark","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
