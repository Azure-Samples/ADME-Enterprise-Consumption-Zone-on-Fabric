{"cells":[{"cell_type":"markdown","id":"f2a2af66","metadata":{},"source":["# Export of Azure Data Manager for Energy (ADME) data to Fabric\n","\n","## External dependencies\n","ADME instance\n","Azure KeyVault to keep secret needed to connect to ADME \n","\n","## Setup of Delta Tables\n","A separate Notebook called \"ADME setup and testing\" has the code to create the delta lake tables for logging, data and last run info\n","\n","## Setup variables for ADME\n","server - They URL to the ADME server. You can find this in the portal on the overview page of the ADME instance\n","\n","Api URLs - These you do not have to change and can use the values already there.\n","\n","data_partition - The name of the data partition you want to export from. Data partitions can be found in the Portal on the ADME instance. Find data partitions on the left side when you have opened the ADME instance.\n","\n","legal_tag - This is the default value that will be put on the exported document if the original does not have a legal_tag\n","\n","acl_viewer - This is the default value that will be put on the exported document if the original does not have a value\n","\n","acl_owner - This is the default value that will be put on the exported document if the original does not have a value\n","\n","authentication_mode - In this example we use \"msal_interactive\" See (insert link here) for more information\n","\n","authority - \"https://login.microsoftonline.com/xxxxx\" where xxxx is your tenantid\n","\n","scopes - [\"xxxx/.default\"], where xxxx is your client_id. NOTE this variable is a list and therefore it needs the square brackets even if it is only one value\n","\n","client_id - this is the app id that was used to create the ADME instance\n","\n","tenant_id - the tenant id. Search Tenant properties in Portal to find this value. This is the tenant where ADME resides.\n","\n","redirect_uri - this value is set on the app used when creating the ADME instance. \n","\n","access_token_type - \"keyvault\", it is strongly recommended that you use a keyvault for the key to access AMDE\n","\n","key_vault_name - the name of the key vault\n","\n","secret_name - the name of the secret in the key vault\n","\n","table_name - the name of the table you store data to in Fabric - put code at bottom of notebook\n","\n","logging_table - the name of the table where logs will be stored\n","\n","run_info_table - the name of the table where last run info is stored. This is used for delta loads since last run time\n","\n","lakehouse_name - the name of the lakehouse where the delta tables will reside\n","\n","## How to use the variables\n","\n","**The variables are called with config[\"variable\"], for example: config[\"server\"]**\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"06e7cf1e-94f7-426b-963c-b3dfcc0d90e9","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","# Correct the JSON string format and load it\n","config_json = '''\n","{\n","    \"server\": \"https://<adme instance>.energy.azure.com\",\n","    \"crs_catalog_url\": \"/api/crs/catalog/v2/\",\n","    \"crs_converter_url\": \"/api/crs/converter/v2/\",\n","    \"entitlements_url\": \"/api/entitlements/v2/\",\n","    \"file_url\": \"/api/file/v2/\",\n","    \"legal_url\": \"/api/legal/v1/\",\n","    \"schema_url\": \"/api/schema-service/v1/\",\n","    \"search_url\": \"/api/search/v2/\",\n","    \"storage_url\": \"/api/storage/v2/\",\n","    \"storage_search_type\":\"query/records:batch\",\n","    \"search_api_search_type\":\"query_with_cursor\",\n","    \"unit_url\": \"/api/unit/v3/\",\n","    \"workflow_url\": \"/api/workflow/v1/\",\n","    \"data_partition_id\": \"\",\n","    \"legal_tag\": \"\",\n","    \"acl_viewer\": \"\",\n","    \"acl_owner\": \"\",\n","    \"authentication_mode\": \"msal_interactive\",\n","    \"authority\": \"https://login.microsoftonline.com/3aa4a235-b6e2-48d5-9195-7fcf05b459b0\",\n","    \"scopes\": [\"\"],\n","    \"client_id\": \"\",\n","    \"tenant_id\": \"\",\n","    \"redirect_uri\": \"http://localhost:8080\",\n","    \"access_token_type\" : \"\",\n","    \"key_vault_name\" : \"\",\n","    \"secret_name\" : \"\",\n","    \"table_name\" : \"\",\n","    \"logging_table\" : \"\",\n","    \"run_info_table\" : \"\",\n","    \"lakehouse_name\" : \"\"\n","}\n","'''\n","\n","\n","# Load the JSON string into a Python dictionary\n","config_dict = json.loads(config_json)\n","\n","# Create a Series from the dictionary\n","config = pd.Series(config_dict)\n","\n","#The number of documents in each batch. If you increase this you could see error messages about the load being too big\n","batch_size = 1000\n","\n","display(config)"]},{"cell_type":"markdown","id":"c7a752a5","metadata":{},"source":["# Spark settings\n","In the cell below you can define the spark environment. Go to **Workspaces - Choose your workspace and then Settings** in the upper right corner of the screen. Under Data Engineerin/Data Science you can find pool information. There you can see how many nodes you have availabe and can use in the settings below (maxExecutors, minExecutors, startExecutors).\n","\n","## Other settings to consider \n","The code will run fine without these, but they can be used to fine tune based on your needs\n","\n","\n","**spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")**\n","\n","Enables the vectorized Parquet reader in Apache Spark. This setting enhances the performance of reading Parquet files by processing data in batches, taking advantage of the columnar storage format and reducing the overhead associated with row-by-row processing. This can lead to faster query execution and more efficient CPU usage, making it a valuable configuration for optimizing big data processing workflows in Spark.\n","\n","**spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")**\n","\n","Enables optimized writing for Delta Lake operations. This optimization can significantly improve write performance by batching small files, reducing I/O operations, and better utilizing resources. It is particularly useful for scenarios involving frequent writes to Delta tables, helping to mitigate issues like the small file problem and ensuring efficient data management.\n","\n","**spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")**\n","\n","Sets the target bin size for the optimized write feature in Delta Lake to 1 GB. This helps in controlling the size of the output files, improving both write and read performance by reducing the number of small files and achieving more consistent file sizes. This setting is particularly useful in optimizing storage and compute resource utilization in environments like Azure Synapse and Azure Databricks.\n","\n","**spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")**\n","\n","Configures Apache Spark to use 200 partitions during shuffle operations. This setting is crucial for tuning the performance of Spark applications, as it affects how data is distributed and processed across the cluster. Adjusting the number of shuffle partitions can help balance the workload, improve parallelism, and optimize resource utilization based on the specific requirements of your data processing tasks.\n","\n","**spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")**\n","\n","Enables the auto-compaction feature for Delta tables in Databricks. This feature helps manage the small file problem by automatically combining small files into larger ones, improving both read and write performance, optimizing storage efficiency, and enhancing overall system performance. Auto-compaction is a valuable feature for maintaining the health and performance of Delta Lake tables, especially in environments with frequent data writes."]},{"cell_type":"markdown","id":"4060eaa3-70b4-413d-a4e7-af374fb6d61f","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Spark Settings"]},{"cell_type":"code","execution_count":null,"id":"cdbcaa0a-6771-4f16-a022-0dac227793f9","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# import packages\n","from pyspark.sql import SparkSession\n","import requests\n","import json\n","import os\n","from msal import ConfidentialClientApplication\n","import pandas as pd\n","from pyspark.sql.types import StructType, StructField, StringType, MapType, TimestampType, IntegerType\n","from pyspark.sql.functions import to_timestamp, to_json, from_json, explode, col, first, current_timestamp\n","from trident_token_library_wrapper import PyTridentTokenLibrary as tl\n","import time\n","import logging\n","import requests\n","from requests.adapters import HTTPAdapter\n","from urllib3.util.retry import Retry\n","import time\n","from pyspark.sql.functions import to_timestamp, to_json, year, month, dayofmonth\n","from delta import DeltaTable\n","\n","# Initialize Spark Session\n","#Changed maxExecutors should be based on your available resources in Fabric\n","spark = SparkSession.builder \\\n","    .appName(\"FullLoad\") \\\n","    .config(\"spark.executor.memory\", \"16g\") \\\n","    .config(\"spark.executor.memoryOverhead\", \"4g\") \\\n","    .config(\"spark.driver.memory\", \"16g\") \\\n","    .config(\"spark.driver.memoryOverhead\", \"4g\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n","    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n","    .config(\"spark.dynamicAllocation.minExecutors\", \"4\") \\\n","    .config(\"spark.dynamicAllocation.maxExecutors\", \"10\") \\\n","    .config(\"spark.dynamicAllocation.initialExecutors\", \"5\") \\\n","    .config(\"spark.eventLog.enabled\", \"true\") \\\n","    .config(\"spark.eventLog.compress\", \"true\") \\\n","    .config(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\\\n","    .config(\"spark.sql.parquet.vorder.enabled\", \"true\")\\\n","    .config(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\\\n","    .config(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")\\\n","    .config(\"spark.sql.files.maxPartitionBytes\", \"512MB\") \\\n","    .config(\"spark.sql.shuffle.partitions\", 200) \\\n","    .getOrCreate()\n","\n","\n","\n","# Configure logging at the beginning\n","logging.basicConfig(level=logging.WARN, format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s')\n"]},{"cell_type":"markdown","id":"668e0f4e-89ed-428c-b14b-73bf747d9cb9","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["\n","# Schemas\n","The solution uses three schemas. One for logging, one for the exported data and one for information on last export. The schemas are listed in the code cell below."]},{"cell_type":"code","execution_count":null,"id":"b38e7d1c-6a07-43bd-ba52-29f2bb9ca783","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Define the log schema\n","log_schema = StructType([\n","    StructField(\"log_id\", StringType(), False),\n","    StructField(\"log_timestamp\", TimestampType(), False),\n","    StructField(\"log_level\", StringType(), False),\n","    StructField(\"file_name\", StringType(), False),\n","    StructField(\"line_number\", StringType(), False),\n","    StructField(\"message\", StringType(), False)\n","])\n","\n","schema = StructType([\n","    StructField(\"data\", MapType(StringType(), StringType()), True),\n","    StructField(\"meta\", StringType(), True),\n","    StructField(\"id\", StringType(), True),\n","    StructField(\"version\", StringType(), True),\n","    StructField(\"kind\", StringType(), True),\n","    StructField(\"acl\", MapType(StringType(), StringType()), True),\n","    StructField(\"legal\", MapType(StringType(), StringType()), True),\n","    StructField(\"tags\", MapType(StringType(), StringType()), True),\n","    StructField(\"createUser\", StringType(), True),\n","    StructField(\"createTime\", StringType(), True)\n","])"]},{"cell_type":"markdown","id":"cb2ec7f0-087a-43b3-8a15-2a7b830095ca","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Logging\n","Below is the schema for logging information and errors in the code to the logging_info table. Example of use: **log_message(\"INFO\", message)**. The logging code will automatically add line number, timestamp and an unique id. The log will also be printed in the notebook.\n"]},{"cell_type":"code","execution_count":null,"id":"d5ad9c16-b586-4158-8e91-efb244988e31","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import current_timestamp, col, lit\n","from pyspark.sql.types import TimestampType, StringType, StructType, StructField\n","import uuid\n","import inspect\n","import logging\n","from threading import Lock\n","from datetime import datetime\n","\n","# Initialize log batch and lock\n","log_batch = []\n","log_lock = Lock()\n","\n","# Define log_message function\n","def log_message(level, message):\n","    global log_batch\n","    \n","    # Log to console immediately\n","    if level == \"INFO\":\n","        print(f\"INFO: {message}\")\n","        logging.info(message)\n","    elif level == \"WARNING\":\n","        print(f\"WARNING: {message}\")\n","        logging.warning(message)\n","    elif level == \"ERROR\":\n","        print(f\"ERROR: {message}\")\n","        logging.error(message)\n","    else:\n","        print(f\"DEBUG: {message}\")\n","        logging.debug(message)\n","\n","    # Create log entry\n","    log_id = str(uuid.uuid4())\n","    frame = inspect.currentframe().f_back\n","    file_name = frame.f_code.co_filename\n","    line_number = frame.f_lineno\n","    log_timestamp = datetime.utcnow()\n","\n","    log_entry = (log_id, log_timestamp, level, file_name, str(line_number), message)\n","\n","    # Add log entry to batch\n","    with log_lock:\n","        log_batch.append(log_entry)\n","\n","\n","# Write all logs to Delta table at the end\n","def write_log_batch_to_delta():\n","    global log_batch\n","    with log_lock:\n","        if log_batch:\n","            # Convert log batch to DataFrame\n","            log_df = spark.createDataFrame(log_batch, schema=log_schema)\n","\n","            # Define the paths for Delta tables\n","            table_name = config[\"logging_table\"]\n","            table_path = f\"Tables/{table_name}\"\n","\n","            # Write the logs to the Delta table in one batch\n","            log_df.write.format(\"delta\").mode(\"append\").save(table_path)\n","\n","            # Clear the log batch\n","            log_batch = []\n","    print(\"INFO: Logging finished\")\n","\n","# OSDU search function\n","import time\n","from requests.exceptions import HTTPError\n","\n","def osdu_search_by_cursor(server: str, search_api: str, access_token: str, partition_id: str, query: dict, search_type: str, max_retries=5):\n","    \n","    search_api = f\"{server}{search_api}{search_type}\"\n","    \n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"data-partition-id\": partition_id,\n","        \"Content-Type\": \"application/json\"\n","    }\n","\n","    retry_strategy = Retry(\n","        total=max_retries,\n","        status_forcelist=[429, 500, 502, 503, 504],\n","        allowed_methods=[\"GET\", \"POST\"],\n","        backoff_factor=1  # Initial backoff factor (will be multiplied exponentially)\n","    )\n","\n","    session = requests.Session()\n","    adapter = HTTPAdapter(max_retries=retry_strategy)\n","    session.mount('http://', adapter)\n","    session.mount('https://', adapter)\n","\n","    retries = 0\n","    backoff_time = 1  # Start with 1 second\n","\n","    while retries < max_retries:\n","        try:\n","            #log_message(\"INFO\", f\"Sending request to {search_api} with query: {json.dumps(query)} and cursor: {query.get('cursor')}\")\n","            response = session.post(search_api, headers=headers, json=query)\n","            response.raise_for_status()  # Raises error for bad HTTP responses\n","            json_response = response.json()\n","\n","            # Handle both Search API (results) and Storage API (records) responses\n","            if 'results' in json_response:\n","                #log_message(\"INFO\", \"Search API returned results.\")\n","                return json_response\n","            elif 'records' in json_response:\n","                #log_message(\"INFO\", \"Storage API returned records.\")\n","                return json_response\n","            else:\n","                error_message = f\"Invalid response content: {json_response}\"\n","                log_message(\"ERROR\", error_message)\n","                return None\n","\n","        except HTTPError as e:\n","            if response.status_code == 429:\n","                retries += 1\n","                log_message(\"ERROR\", f\"Received 429 Too Many Requests. Retry {retries}/{max_retries} after {backoff_time} seconds.\")\n","                retry_after = response.headers.get('Retry-After')\n","                if retry_after:\n","                    backoff_time = int(retry_after)\n","                else:\n","                    backoff_time = min(2 ** retries, 60)  # Exponential backoff, cap at 60 seconds\n","                time.sleep(backoff_time)\n","            else:\n","                error_message = f\"HTTP Error: {e}, URL: {search_api}, Query: {json.dumps(query)}\"\n","                log_message(\"ERROR\", error_message)\n","                return None\n","        except requests.exceptions.ConnectionError as e:\n","            log_message(\"ERROR\", f\"Connection Error: {e}, URL: {search_api}\")\n","            return None\n","        except requests.exceptions.Timeout as e:\n","            log_message(\"ERROR\", f\"Timeout Error: {e}, URL: {search_api}\")\n","            return None\n","        except Exception as e:\n","            log_message(\"ERROR\", f\"Unexpected error: {e}, URL: {search_api}\")\n","            return None\n","\n","    log_message(\"ERROR\", f\"Max retries exceeded for {search_api}\")\n","    return None\n"]},{"cell_type":"markdown","id":"50e795d9-778c-48a1-950b-0144e07d4f37","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Authentication"]},{"cell_type":"code","execution_count":null,"id":"230d9ad5-72dc-482d-81e5-16df9b4bc792","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from msal import ConfidentialClientApplication\n","# Authentication function\n","def authenticate_osdu(client_id: str, client_secret: str, authority: str, scopes: list):\n","    try:\n","        app = ConfidentialClientApplication(\n","            client_id=client_id,\n","            client_credential=client_secret,\n","            authority=authority\n","        )\n","\n","        result = app.acquire_token_for_client(scopes=scopes)\n","        \n","        if \"access_token\" in result:\n","            message = \"Authentication successful\"\n","            log_message(\"INFO\", message)\n","            return result['access_token']\n","        else:\n","            error_message = f\"Authentication failed: {result.get('error')}, {result.get('error_description')}\"\n","            log_message(\"ERROR\", error_message)\n","    except Exception as e:\n","        error_message = f\"Unexpected error during authentication: {e}\"\n","        log_message(\"ERROR\", error_message)\n","    return None\n","\n","key_vault_name = config[\"key_vault_name\"]\n","access_token = authenticate_osdu(\n","    client_id = config['client_id'],    \n","    client_secret= tl.get_secret_with_token(\n","        f\"https://{key_vault_name}.vault.azure.net/\",\n","        config[\"secret_name\"],\n","        mssparkutils.credentials.getToken(config[\"access_token_type\"])\n","    ),\n","    authority= config['authority'],\n","    scopes= config['scopes']\n","    \n",")\n","\n","\n"]},{"cell_type":"markdown","id":"b4ae120b-b759-4efb-a306-1aa4afa90170","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Creating the table"]},{"cell_type":"code","execution_count":null,"id":"ed39f923-5fc1-4ff3-99fe-e382282db29f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["###This code only needs to be run once to establish the table where the data will be stored\n","rom pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n","from delta.tables import DeltaTable\n","\n","# Initialize Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"LoggingTableCreation\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","    .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\") \\\n","    .getOrCreate()\n","\n","# Function to overwrite Delta table schema\n","def recreate_table_with_new_schema(table_name, schema):\n","    # Resolve the table path from config\n","    table_path = f\"Tables/{table_name}\"\n","    print(f\"Processing table: {table_name}, Path: {table_path}\")\n","    \n","    # Check if the table exists\n","    table_exists = DeltaTable.isDeltaTable(spark, table_path)\n","    print(f\"Table exists: {table_exists}, Path: {table_path}\")\n","\n","    # Overwrite the table with the new schema\n","    empty_df = spark.createDataFrame([], schema)\n","    empty_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\"overwrite\").save(table_path)\n","\n","    print(f\"Table {table_name} created or updated at {table_path} with the new schema.\")\n","\n","# Example usage\n","# Assuming table_name and storage_schema are loaded from config\n","table_name = config[\"table_name\"]\n","storage_schema = StructType([\n","    StructField(\"data\", StringType(), True),\n","    StructField(\"meta\", StringType(), True),\n","    StructField(\"id\", StringType(), True),\n","    StructField(\"version\", StringType(), True),\n","    StructField(\"kind\", StringType(), True),\n","    StructField(\"acl\", StringType(), True),\n","    StructField(\"legal\", StringType(), True),\n","    StructField(\"tags\", StringType(), True),\n","    StructField(\"createUser\", StringType(), True),\n","    StructField(\"createTime\", TimestampType(), True),\n","    StructField(\"ingestTime\", TimestampType(), True)\n","])\n","\n","# Call the function with table name from config\n","recreate_table_with_new_schema(table_name, storage_schema)\n"]},{"cell_type":"markdown","id":"55a8e3bf-152d-4560-b8aa-1f22eccb5101","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Reset last run time manually"]},{"cell_type":"code","execution_count":null,"id":"c65e9ebc-b2a6-426c-baee-8f0ca271cf0b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["## Added code Jon Olav Abeland 24.05. This code will create a table to store last run value for delta\n","## The purpose of this code is to reset last run time so that it is possible to rerun the code for testing multiple times\n","## This code can be safely run as it only creates a table if none exists\n","#from pyspark.sql import SparkSession\n","from pyspark.sql.functions import current_timestamp, lit, unix_timestamp\n","import uuid\n","from datetime import datetime, timezone\n","\n","# Initialize Spark session (uncomment if needed)\n","# spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()\n","\n","lakehouse_name = config[\"lakehouse_name\"]\n","table_name = f\"{lakehouse_name}.run_info\"\n","\n","# SQL to create the table if it doesn't exist\n","create_table_sql = f\"\"\"\n","CREATE TABLE IF NOT EXISTS {lakehouse_name}.run_info (\n","    run_id STRING,\n","    run_timestamp LONG\n",")\n","\"\"\"\n","\n","# Execute the SQL to create the table\n","spark.sql(create_table_sql)\n","\n","# Clear the table for testing purposes (remove this in production)\n","spark.sql(f\"DELETE FROM {lakehouse_name}.run_info\")\n","\n","# Define the specific test date\n","test_run_date = \"2024-09-05 00:00:00\"\n","\n","# Insert the test run date as epoch timestamp\n","run_id = str(uuid.uuid4())\n","run_info_df = spark.createDataFrame([(run_id,)], [\"run_id\"])\n","# Convert to epoch time in milliseconds by multiplying the unix timestamp by 1000\n","run_info_df = run_info_df.withColumn(\"run_timestamp\", (unix_timestamp(lit(test_run_date)).cast(\"long\")*1000000))\n","print(run_info_df.collect()[0])\n","\n","run_info_df.write.insertInto(table_name, overwrite=False)\n","\n","# Query to get the last run value\n","last_run_df = spark.sql(f\"\"\"\n","SELECT run_timestamp\n","FROM {lakehouse_name}.run_info\n","ORDER BY run_timestamp DESC\n","LIMIT 1\n","\"\"\")\n","\n","# Collect the result and extract the timestamp\n","last_run_timestamp = last_run_df.collect()[0]['run_timestamp']\n","\n","print(f\"Last run timestamp: {last_run_timestamp}\")\n","\n","\n","# Convert epoch time back to human-readable format\n","# Assuming last_run_timestamp is in milliseconds\n","#human_readable_timestamp = datetime.utcfromtimestamp(last_run_timestamp / 1000000).strftime('%Y-%m-%d %H:%M:%S')\n","human_readable_timestamp = datetime.fromtimestamp(last_run_timestamp / 1000000, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n","print(f\"Last run timestamp: {human_readable_timestamp}\")\n"]},{"cell_type":"markdown","id":"609a19eb-b27f-423a-a995-8b52f57f2cbf","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Main code\n","\n","## Performance\n",".config(\"spark.sql.files.maxPartitionBytes\", \"512MB\") is set in order to make fewer and larger parquet files to improve performance when using the data later, and possibly improve performance for the upcert of new data with my code. \n","\n","The Search API takes 1000 documents in each request in about one second, then ids from this are used to in Storage API in batches of 20 before the data from the Storage API is written to the DataLake. If I run this in order it takes about one minute to load 100 documents. \n","Since the Search API is fast, this code is not optimized. In order to avoid writing the Storage metadata to the DeltaLake 20 documents at a time, there is a buffer that will only write to the DeltaLake once 1000 documents are collected. The writing is also done separately from the Storage API processes. These changes save a lot of time and are about three times faster. \n","\n","The code uses threads in order to call the Storage API in parallell. It seems like there is not much performance improvement beyond 4 processess. This is probably due to the number of nodes available and/or the Storage API forcing the code to back off due to 429 (too many requests). I suggest that 4 threads should be used for the code. The reason that \"Writing to Delta\" is 0 seconds in the image below for 4 and 8 threads is that it is able to write in parallell with the Storage API calls and do not add time. "]},{"cell_type":"code","execution_count":null,"id":"2b1fcba0-9736-463b-8382-df2b9e9f5f8c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark import SparkContext, SparkConf\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import time\n","from datetime import datetime, timezone\n","from delta import DeltaTable\n","from pyspark.sql.utils import AnalysisException\n","from pyspark.sql.functions import unix_timestamp, current_timestamp, lit\n","import uuid\n","from threading import Thread\n","from queue import Queue\n","\n","import time\n","from concurrent.futures import ThreadPoolExecutor\n","\n","# Global buffer and write lock\n","write_lock = Lock()\n","buffer_df = None  # To accumulate DataFrames\n","write_queue = Queue()  # Queue for buffered writes\n","\n","# Fetch and process Storage API data in batches\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","# Fetch and process Storage API data in parallel batches\n","def fetch_storage_data_in_batches(ids, access_token, batch_size=20, max_workers=2):\n","    processed_count = 0\n","    not_found_ids = []\n","    success = True\n","    search_type = config[\"storage_search_type\"]\n","    search_api = config[\"storage_url\"]\n","\n","    # Executor to run Storage API calls in parallel\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = []\n","\n","        # Submit Storage API calls to be processed in parallel\n","        for i in range(0, len(ids), batch_size):\n","            batch_ids = ids[i:i + batch_size]\n","            storage_query = {\"records\": batch_ids}\n","\n","            # Schedule the storage API call\n","            futures.append(executor.submit(make_api_call, storage_query, access_token, search_type, search_api))\n","\n","        # As each future completes, process the results\n","        for future in as_completed(futures):\n","            try:\n","                storage_response = future.result()\n","                if storage_response:\n","                    # Process the batch when the Storage API call completes\n","                    success = process_storage_batch_with_retry(storage_response) and success\n","\n","                    # Update processed count and print every 100 documents\n","                    processed_count += len(storage_response['records'])\n","                    if processed_count % 100 == 0:\n","                        print(f\"Processed {processed_count} documents so far...\")\n","                else:\n","                    print(f\"Failed to fetch data for one of the batches.\")\n","                    success = False\n","            except Exception as e:\n","                print(f\"Error during batch processing: {e}\")\n","                success = False\n","\n","    if not_found_ids:\n","        print(f\"IDs not found in Storage API: {', '.join(not_found_ids)}\")\n","\n","    return success\n","\n","# Function to make the API call\n","def make_api_call(query, access_token, search_type, search_api):\n","    try:\n","        #log_message(\"INFO\", f\"Making API call with payload: {json.dumps(query)}\")\n","        response = osdu_search_by_cursor(\n","            server=config['server'],\n","            search_api=search_api,\n","            access_token=access_token,\n","            partition_id=config['data_partition_id'],\n","            query=query,\n","            search_type=search_type\n","        )\n","        if response:\n","            if 'results' in response:\n","                return response\n","            elif 'records' in response:\n","                return response\n","            else:\n","                log_message(\"ERROR\", f\"Unexpected response format: {json.dumps(response)}\")\n","                return None\n","        else:\n","            log_message(\"ERROR\", \"Empty response from API\")\n","            return None\n","    except requests.exceptions.RequestException as e:\n","        log_message(\"ERROR\", f\"RequestException: {e}\")\n","        if e.response:\n","            log_message(\"ERROR\", f\"HTTP Error: {e.response.status_code} - {e.response.text}\")\n","        return None\n","\n","# Function to buffer data and trigger async write when buffer is full\n","def buffer_and_queue_write(unique_df, buffer_size=2000):\n","    global buffer_df\n","    if buffer_df is None:\n","        buffer_df = unique_df\n","    else:\n","        buffer_df = buffer_df.union(unique_df)\n","\n","    if buffer_df.count() >= buffer_size:\n","        # Send buffer to queue for async writing and reset buffer\n","        write_queue.put(buffer_df)\n","        buffer_df = None\n","\n","# Function to write to Delta Lake from queue\n","def async_writer():\n","    while True:\n","        df = write_queue.get()  # Get DataFrame from the queue\n","        if df is None:\n","            break  # Sentinel value to exit the thread\n","        write_to_delta(df)\n","\n","# Function to actually write to Delta Lake\n","def write_to_delta(df):\n","    with write_lock:\n","        table = DeltaTable.forPath(spark, table_path)\n","        table.alias(\"table\") \\\n","            .merge(\n","                df.alias(\"updates\"),\n","                \"table.id = updates.id\"\n","            ) \\\n","            .whenMatchedUpdateAll() \\\n","            .whenNotMatchedInsertAll() \\\n","            .execute()\n","\n","# Modified process_storage_batch_with_retry to use buffer_and_queue_write\n","def process_storage_batch_with_retry(storage_response, max_retries=3):\n","    retries = 0\n","    while retries < max_retries:\n","        try:\n","            # Create DataFrame from the Storage API response\n","            df = spark.createDataFrame(storage_response['records'], schema)\n","            unique_df = df.dropDuplicates([\"id\"])\n","\n","            # Apply necessary transformations (createTime and ingestTime)\n","            if 'createTime' in df.columns:\n","                unique_df = unique_df.withColumn(\"createTime\", to_timestamp(col(\"createTime\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\n","\n","            unique_df = unique_df.withColumn(\"ingestTime\", current_timestamp())\n","            unique_df = unique_df.withColumn(\"legal\", to_json(col(\"legal\"))) \\\n","                                 .withColumn(\"acl\", to_json(col(\"acl\"))) \\\n","                                 .withColumn(\"tags\", to_json(col(\"tags\"))) \\\n","                                 .withColumn(\"data\", to_json(col(\"data\")))\n","\n","            # Buffer the data and queue for async writing\n","            buffer_and_queue_write(unique_df)\n","\n","            return True\n","        except AnalysisException as e:\n","            if \"concurrent update\" in str(e):\n","                retries += 1\n","                print(f\"Concurrent update detected. Retry {retries} of {max_retries}.\")\n","                time.sleep(2 ** retries)\n","            else:\n","                print(f\"Failed to process batch: {e}\")\n","                break\n","    print(f\"Failed to process batch after {max_retries} retries\")\n","    return False\n","\n","# Async writer thread to handle Delta writes\n","writer_thread = Thread(target=async_writer, daemon=True)\n","writer_thread.start()\n","\n","# After all API calls, flush any remaining data in the buffer and stop the writer\n","def stop_async_writer():\n","    global buffer_df\n","    # Check if there are any remaining records in the buffer\n","    if buffer_df is not None and buffer_df.count() > 0:\n","        print(f\"Flushing remaining {buffer_df.count()} documents from buffer...\")\n","        write_queue.put(buffer_df)  # Flush remaining data in the buffer\n","        buffer_df = None  # Reset the buffer\n","\n","    # Signal the async writer to stop by adding a sentinel value\n","    write_queue.put(None)\n","    writer_thread.join()\n","\n","\n","# Fetch and process documents with a document limit\n","def sequential_api_calls_with_parallel_processing(cursor, access_token, query, document_limit=None):\n","    ids = []\n","    total_count = 0\n","    first_call = True\n","    search_type = config[\"search_api_search_type\"]\n","    search_api = config[\"search_url\"]\n","\n","    while True:\n","        response = make_api_call(query, access_token, search_type, search_api)\n","        if response:\n","            if first_call:\n","                total_count = response.get('totalCount', len(response['results']))\n","                print(f\"Total documents found: {total_count}\")\n","                first_call = False\n","\n","            for doc in response['results']:\n","                if document_limit and len(ids) >= document_limit:\n","                    print(f\"Reached document limit: {document_limit}\")\n","                    break\n","                ids.append(doc['id'])\n","\n","            # Stop processing if document limit is reached\n","            if document_limit and len(ids) >= document_limit:\n","                break\n","\n","            cursor = response.get('cursor')\n","            if cursor:\n","                query['cursor'] = cursor\n","            else:\n","                print(f\"All pages processed. Total documents processed: {len(ids)}\")\n","                break\n","        else:\n","            print(\"Search API call failed!\")\n","            return False\n","\n","    # Process fetched IDs in batches\n","    if ids:\n","        success = fetch_storage_data_in_batches(ids, access_token)\n","    else:\n","        print(\"No IDs extracted from the Search API response\")\n","        success = False\n","\n","    # Stop the async writer after all processing\n","    stop_async_writer()\n","\n","    return success\n","\n","# Main code\n","print(\"Batch export started\")\n","\n","# Define paths for Delta tables\n","table_name = config[\"table_name\"]\n","table_path = f\"Tables/{table_name}\"\n","lakehouse_name = config[\"lakehouse_name\"]\n","\n","# Query to get the last run value\n","last_run_df = spark.sql(f\"\"\"\n","SELECT run_timestamp\n","FROM {lakehouse_name}.run_info\n","ORDER BY run_timestamp DESC\n","LIMIT 1\n","\"\"\")\n","\n","# Check if the last run timestamp exists\n","if last_run_df.count() == 0:\n","    last_run_date = 0  # Default to epoch if no previous run\n","else:\n","    last_run_date = last_run_df.collect()[0]['run_timestamp']\n","\n","###last_run_date is epoch/version\n","query = {\n","    \"kind\": \"*:*:*:*\",\n","    \"query\": f\"version:[{last_run_date} TO *]\",\n","    \"limit\": batch_size\n","}\n","\n","# Set a document limit for testing performance\n","document_limit = 2000  # Set this to the number of documents you'd like to test\n","\n","search_type = config[\"search_api_search_type\"]\n","search_api = config[\"search_url\"]\n","\n","\n","print(f\"Last run date (epoch): {last_run_date}\")\n","human_readable_timestamp = datetime.fromtimestamp(last_run_date / 1000000, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n","print(f\"Last run timestamp: {human_readable_timestamp}\")\n","\n","# Function to update the last run timestamp in the run_info table\n","def update_last_run_timestamp(lakehouse_name):\n","    # Delete existing run_info records to keep only the latest run\n","    #spark.sql(f\"DELETE FROM {lakehouse_name}.run_info\")\n","\n","    # Insert the current timestamp in epoch format as the new last run timestamp\n","    run_id = str(uuid.uuid4())\n","    run_info_df = spark.createDataFrame([(run_id,)], [\"run_id\"])\n","    run_info_df = run_info_df.withColumn(\"run_timestamp\", (unix_timestamp(lit(current_timestamp())).cast(\"long\") * 1000000))\n","    #run_info_df.write.insertInto(f\"{lakehouse_name}.run_info\", overwrite=False)\n","    log_message(\"INFO\", \"Last run timestamp updated successfully\")\n","\n","# Function to process the main data pipeline\n","def main_process(access_token, query, reset_last_run=False, document_limit=None):\n","    # Make the initial API call to get the cursor and total count\n","    response = make_api_call(query, access_token, search_type, search_api)\n","    \n","    if response:\n","        cursor = response.get('cursor')\n","        total_count = response.get('totalCount', 0)\n","        log_message(\"INFO\", f\"Documents found: {total_count}\")\n","\n","        # Start processing batches sequentially with document limit\n","        success = sequential_api_calls_with_parallel_processing(cursor, access_token, query, document_limit=document_limit)\n","\n","        if success:\n","            lakehouse_name = config[\"lakehouse_name\"]\n","\n","            # Reset last run timestamp if required\n","            if reset_last_run:\n","                log_message(\"INFO\", \"Resetting the last run timestamp...\")\n","                update_last_run_timestamp(lakehouse_name)\n","            else:\n","                # Update the last run timestamp if processing was successful\n","                log_message(\"INFO\", \"Updating the last run timestamp after successful processing...\")\n","                update_last_run_timestamp(lakehouse_name)\n","        else:\n","            log_message(\"ERROR\", \"Batch processing failed. Last run timestamp not updated.\")\n","    else:\n","        log_message(\"ERROR\", \"Initial API call failed.\")\n","\n","    # Ensure the logging process completes\n","    write_log_batch_to_delta()\n","    log_message(\"INFO\", \"write_log_batch_to_delta() executed successfully\")\n","\n","# Assuming access_token, query, and other configurations are set correctly\n","###make document limin write last run time. Set time value from last document as last run time\n","main_process(access_token=access_token, query=query, reset_last_run=True, document_limit=20000000000000)  # Example with document limit\n"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"e6f6b090-d466-4a54-ac28-a3e39b83b795","default_lakehouse_name":"bronze","default_lakehouse_workspace_id":"f974bf36-32b5-44a7-9eab-79abe20d0b42"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
